# Statistics for Data Science

Sheil Sarda `<sheils@seas.upenn.edu>`

Taught by Prof. Hamed Hassani in Fall 2021.

## Course Description

The course covers the methodological foundations of data science, emphasizing basic concepts in statistics and learning theory, but also modern methodologies. Learning of distributions and their parameters. Testing of multiple hypotheses. Linear and nonlinear regression and prediction. Classification. Uncertainty quantification. Model validation. Clustering. Dimensionality reduction. Probably approximately correct (PAC) learning. Such theoretical concepts are further complemented by exemplar applications, case studies (datasets), and programming exercises (in Python) drawn from electrical engineering, computer science, the life sciences, finance, and social networks.

## Syllabus

### Week 1. 
Histograms, empirical distributions. Plug in estimators.

### Week 2. 
Confidence intervals. Parameter estimation. Method of moments.

### Week 3 & 4. 
Maximum likelihood estimators and their fundamental properties.  The Cramr-Rao bound.

### Week 5. 
The concepts of sufficiency. The Rao-Blackwell theorem. Bayesian methods.

### Week 6. 
Hypothesis testing. Basics: t-test and z-test. p value. Generalized likelihood ratio test.

### Week 7.  
Optimality properties of tests. Neyman-Pearson Lemma.

### Week 8. 
Formulation of the regression problem. Simple linear regression. Test on regression coefficient. Cross-validation.


### Week 9. 
Formulation of classification problem. Bayes Optimal Classifier. Logistic regression.  Linear and Quadratic Discriminant Analysis. K-Nearest Neighbors.

### Week 11. 
SVD and PCA

### Week 12. 
Data Clustering and the K-means algorithm

### Week 13. 
A formal model: The statistical learning framework. Empirical risk minimization (ERM). The basic pitfalls: “No free lunch” principle.  PAC learning.

### Week 14.  
Introduction to VC theory.


